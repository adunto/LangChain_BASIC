{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe2cf216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e80c9e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['input'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='당신은 개발자입니다.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [ (\"system\", \"당신은 개발자입니다.\") , \n",
    "     (\"user\", \"{input}\")]\n",
    ")\n",
    "print(prompt)\n",
    "\n",
    "prompt_text = prompt.format(input=\"JavaScript는 무엇인가요? 자세하게 설명해주세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9db56cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x000001F314225E50> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001F314296F60> root_client=<openai.OpenAI object at 0x000001F3114D4530> root_async_client=<openai.AsyncOpenAI object at 0x000001F313D9BC20> model_name='meta-llama/llama-4-scout-17b-16e-instruct' temperature=0.7 model_kwargs={} openai_api_key=SecretStr('**********') openai_api_base='https://api.groq.com/openai/v1'\n"
     ]
    }
   ],
   "source": [
    "# Groq API를 사용하는 ChatOpenAI 인스턴스 생성\n",
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02f69c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오류 발생: Error code: 401 - {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = llm.invoke(prompt_text)\n",
    "    print(type(response))\n",
    "    print(response)\n",
    "    print(\"응답:\", response.content)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9444d0a",
   "metadata": {},
   "source": [
    "### LCEL (Prompt + LLM모델)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d707a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='You are an expert in AI Expert. Answer the question. <Question>: {input}에 대해 쉽게 설명해주세요.')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"You are an expert in AI Expert. Answer the question. <Question>: {input}에 대해 쉽게 설명해주세요.\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88c42edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "인공지능 모델의 학습 원리는 크게 두 가지 방법으로 나눌 수 있습니다. 첫 번째는 지도학습이며, 이는 레이블이 달린 데이터를 사용하여 모델을 학습시키는 방법입니다. 모델은 입력 데이터와 해당 입력 데이터에 대한 정확한 출력(레이블)을 학습하여 패턴을 식별하고 예측을 수행합니다. \n",
      "\n",
      "두 번째 방법은 비지도학습입니다. 이는 레이블이 달려있지 않은 데이터를 사용하여 모델을 학습시키는 방법입니다. 모델은 입력 데이터의 패턴을 식별하고 클러스터링, 차원 축소, 이상치 탐지 등의 작업을 수행합니다.\n",
      "\n",
      "두 방법 모두 학습 과정은 데이터를 입력하고 모델이 예측한 출력 값과 실제 값 사이의 오차를 최소화하는 방향으로 가중치를 조정하면서 반복적으로 학습이 진행됩니다. 이를 통해 모델은 데이터의 패턴을 학습하고 새로운 입력에 대한 예측을 수행할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# chain 연결 (LCEL)\n",
    "chain = prompt | llm\n",
    "\n",
    "# chain 호출\n",
    "result = chain.invoke({\"input\": \"인공지능 모델의 학습 원리\"})\n",
    "print(type(result))\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c84345",
   "metadata": {},
   "source": [
    "### LLM 체인\n",
    "- ( Prompt + LLM 모델 + 출력파서) - LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cf639d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='You are an expert in AI Expert. Answer the question. <Question>: {input}에 대해 쉽게 설명해주세요.')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 프롬프트 생성\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"You are an expert in AI Expert. Answer the question. <Question>: {input}에 대해 쉽게 설명해주세요.\"\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "811b05d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d34d0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. chain 생성 (LCEL)\n",
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5feec22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<Answer>:\n",
      "인공지능 모델의 학습원리는 데이터를 입력으로 받아 일련의 계산을 통해 예측을 수행하는 과정입니다. 모델은 입력 데이터와 정답 데이터 사이의 관계를 학습하여 새로운 데이터에 대해 예측을 할 수 있도록 학습됩니다. 학습 과정에서 모델은 오차를 최소화하기 위해 가중치를 조정하며, 이를 통해 데이터 간의 패턴과 관련성을 파악하게 됩니다. 이러한 학습과정을 통해 모델은 새로운 데이터에 대한 예측을 보다 정확하게 수행할 수 있게됩니다.\n"
     ]
    }
   ],
   "source": [
    "# 3. chain의 invoke 호출\n",
    "result = chain.invoke({\"input\": \"인공지능 모델의 학습 원리\"})\n",
    "print(type(result))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cf85923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "LangChain은 다양한 AI 기술과 서비스를 제공하는 기업입니다. LangChain의 제품에는 다국어 번역 및 통역 솔루션, 자연어 처리 기술을 활용한 챗봇 및 가상 비서 서비스, 언어 학습 및 교육 플랫폼 등이 있습니다. 또한 LangChain은 고객의 요구에 맞는 맞춤형 AI 솔루션을 개발하여 제공하고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    result = chain.invoke({\"input\": \"LangChain의 Products(제품)는 어떤 것들이 있나요?\"})\n",
    "    print(type(result))\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0fa10b",
   "metadata": {},
   "source": [
    "### Runnable의 stream() 함수 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ceac380f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object RunnableSequence.stream at 0x000001F30CB5F2E0>\n",
      "인공지능 모델의 학습 원리를 쉽게 설명해 드리겠습니다.\n",
      "\n",
      "인공지능 모델은 데이터를 입력받아 학습을 하고, 그 데이터에서 패턴을 찾아내어 새로운 데이터를 예측하거나 분류하는 작업을 수행합니다. 이 과정은 크게 입력 데이터를 받아들이고, 이를 가공하여 출력하는 과정으로 나눌 수 있습니다.\n",
      "\n",
      "학습 과정은 입력 데이터를 모델에 주입하고, 모델이 주어진 데이터를 분석하여 일정한 패턴이나 특징을 학습합니다. 이때 모델은 사전에 정의된 목표에 맞게 최적화되는데, 이것을 어떤 기준 함수를 사용하여 측정할 수 있습니다.\n",
      "\n",
      "모델이 학습을 통해 데이터를 분류하거나 예측할 수 있는 이유는 이렇게 학습된 패턴을 기반으로 새로운 데이터에 대한 예측을 수행할 수 있기 때문입니다.따라서, 인공지능 모델의 핵심은 데이터를 효과적으로 분석하여 패턴을 찾아내는 것이라고 할 수 있습니다."
     ]
    }
   ],
   "source": [
    "# 스트리밍 출력을 위한 요청\n",
    "try:\n",
    "    answer = chain.stream({\"input\": \"인공지능 모델의 학습 원리를 자세하게 설명해 주세요.\"})\n",
    "    # 스트리밍 출력\n",
    "    print(answer)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "for token in answer:\n",
    "    # 스트림에서 받은 데이터의 내용을 출력합니다. 줄바꿈 없이 이어서 출력하고, 버퍼를 즉시 비웁니다.\n",
    "    print(token, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8080a1",
   "metadata": {},
   "source": [
    "### Multi Chain\n",
    "* 첫 번째 Chain의 출력이, 두 번째 Chain의 입력이 된다.\n",
    "* 두 개의 Chain과 Prompt + OutputParser를 LCEL로 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fc8c973",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Step 1: 사용자가 입력한 장르에 따라 영화 추천\n",
    "prompt1 = ChatPromptTemplate.from_template(\"{genre} 장르에서 추천할 만한 영화를 한 편 알려주세요.\")\n",
    "\n",
    "# Step 2: 추천된 영화의 줄거리를 요약\n",
    "prompt2 = ChatPromptTemplate.from_template(\"{movie} 추전한 영화의 제목을 먼저 알려주시고, 줄을 바꾸어서 영화의 줄거리를 7문장으로 요약해 주세요.\")\n",
    "\n",
    "# OpenAI 모델 사용\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "# 체인 1: 영화 추천 (입력: 장르 → 출력: 영화 제목)\n",
    "chain1 = prompt1 | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "db19765b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'매드 맥스: 분노의 도로'\n",
      "\n",
      "- 줄거리 요약 -\n",
      "\n",
      "맥스는 포스트 아포칼립스 세계에서 자신의 차를 빼앗긴 악당들과의 전투 속에서 미친 듯이 달리며 쫓기게 된다. 그리고 그는 무모한 여성 전사인 퍼리오사와 만나 함께 악당들을 상대로 사상 최악의 전투를 벌이게 된다. 맥스는 물자가 부족한 세상에서 살아가기 위해 불타는 소목에 직면하게 되지만, 그는 자신의 용기와 전투력으로 어려움을 극복해 나가며 포스트 아포칼립스 세계에서 살아남기 위한 모험을 떠난다. 퍼리오사와 함께한 맥스는 악당들과의 치열한 싸움 속에서 삶과 죽음을 건 결전을 펼치게 되는데, 그 과정에서 맥스는 내면의 분노와 욕망, 그리고 소망과 희망을 마주하게 되는데...\n"
     ]
    }
   ],
   "source": [
    "# 체인 2: 줄거리 요약 (입력: 영화 제목 → 출력: 줄거리)\n",
    "chain2 = (\n",
    "    {\"movie\": chain1}  # chain1의 출력을 movie 변수로 전달\n",
    "    | prompt2\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 실행: \"SF\" 장르의 영화 추천 및 줄거리 요약\n",
    "response = chain2.invoke({\"genre\": \"액션\"})\n",
    "print(response)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836b41de",
   "metadata": {},
   "source": [
    "### PromptTemplate 여러개 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406b8e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_text = \"{model_name} 모델의 학습 원리를 {count} 문장으로 한국어로 답변해 주세요.\"\n",
    "\n",
    "# PromptTemplate 인스턴스를 생성\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "# 템플릿에 값을 채워서 프롬프트를 완성\n",
    "filled_prompt = prompt_template.format(model_name=\"ChatGPT\", count=3)\n",
    "\n",
    "# 문자열 템플릿 결합 (PromptTemplate + PromptTemplate + 문자열)\n",
    "combined_prompt = (\n",
    "              prompt_template\n",
    "              + PromptTemplate.from_template(\"\\n\\n 그리고 {model_name} 모델의 장점을 요약 정리해 주세요\")\n",
    "              + \"\\n\\n {model_name} 모델과 비슷한 AI 모델은 어떤 것이 있나요? 모델명은 {language}로 답변해 주세요.\"\n",
    ")\n",
    "combined_prompt.format(model_name=\"ChatGPT\", count=3, language=\"영어\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "chain = combined_prompt | llm | StrOutputParser()\n",
    "response = chain.invoke({\"model_name\":\"ChatGPT\", \"count\":3, \"language\":\"영어\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf05014",
   "metadata": {},
   "source": [
    "### 여러개의 PromptTemplate 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105998cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GPT-4 모델의 학습 원리를 2 문장으로 한국어로 답변해 주세요.', 'Gemma 모델의 학습 원리를 3 문장으로 한국어로 답변해 주세요.']\n"
     ]
    }
   ],
   "source": [
    "template_text = \"{model_name} 모델의 학습 원리를 {count} 문장으로 한국어로 답변해 주세요.\"\n",
    "\n",
    "# PromptTemplate 인스턴스를 생성\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "questions = [\n",
    "    {\"model_name\": \"GPT-4\", \"count\": 2},\n",
    "    {\"model_name\": \"Gemma\", \"count\": 3},\n",
    "]\n",
    "\n",
    "# 여러 개의 프롬프트를 미리 생성\n",
    "formatted_prompts = [prompt_template.format(**q) for q in questions]\n",
    "print(formatted_prompts)  # 미리 생성된 질문 목록 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd5eeb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='GPT-4 모델은 대규모의 데이터셋을 사용하여 자기 지도 학습 방식으로 사전 훈련되며, 텍스트의 다음 단어를 예측하는 과정을 통해 문맥을 이해하고 생성합니다. 이 모델은 엄청난 양의 매개변수와 계산 능력을 가지고 있어 자연어 처리 작업에서 뛰어난 성능을 발휘합니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 132, 'prompt_tokens': 40, 'total_tokens': 172, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BgnWMYdOyniIK1zt4Q7RMrjJXUZCH', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--3df5e478-3a58-4cdf-8a18-4a4e88ae1a13-0' usage_metadata={'input_tokens': 40, 'output_tokens': 132, 'total_tokens': 172, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "content='Gemma 모델은 전이 학습을 통해 기존의 모델을 사용하여 새로운 데이터셋에 대한 학습 성능을 향상시킵니다. Gemma 모델은 학습된 정보를 저장하고 일정 기간 이를 보존하여 학습 시간을 단축시키며, 새로운 데이터에 빠르게 적응할 수 있습니다. Gemma 모델은 강화 학습 알고리즘을 적용하여 에이전트가 환경과 상호작용하며 보상을 최대화하는 방향으로 학습하도록 유도합니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 178, 'prompt_tokens': 38, 'total_tokens': 216, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BgnWPpT09fo8kCjSxWJMaBzfvsC3a', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--aa8f6cc8-6761-4dde-be18-b238d3ca2586-0' usage_metadata={'input_tokens': 38, 'output_tokens': 178, 'total_tokens': 216, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "for prompt in formatted_prompts:\n",
    "    response = llm.invoke(prompt)\n",
    "    print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e81b234",
   "metadata": {},
   "source": [
    "### ChatPromptTemplate\n",
    "- SystemMessagePromptTemplate, HumanMessagePromptTemplate, \n",
    "AIMessagePromptTemplate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ca0ccaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep learning is a subfield of machine learning that involves building and training artificial neural networks with multiple layers (hence the term \"deep\"). These neural networks are designed to automatically learn and represent complex patterns and relationships within data. Deep learning has seen significant advancements in recent years, particularly with the use of deep neural networks known as convolutional neural networks (CNNs) for image recognition, recurrent neural networks (RNNs) for sequential data processing, and transformers for natural language processing tasks. Deep learning has been applied successfully in various domains such as computer vision, speech recognition, natural language processing, and reinforcement learning. Its ability to automatically learn hierarchical representations from data has led to improved performance in tasks that were previously considered challenging for traditional machine learning algorithms.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 개별 메시지 템플릿 정의\n",
    "system_message = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are an {topic} expert. Please provide clear and detailed explanations.\"\n",
    ")\n",
    "user_message = HumanMessagePromptTemplate.from_template(\n",
    "    \"{question}\"\n",
    ")\n",
    "ai_message = AIMessagePromptTemplate.from_template(\n",
    "    \"This is an example answer about {topic}.\"\n",
    ")\n",
    "\n",
    "# ChatPromptTemplate로 메시지들을 묶기\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_message,\n",
    "    user_message,\n",
    "    ai_message\n",
    "])\n",
    "\n",
    "# 메시지 생성\n",
    "messages = chat_prompt.format_messages(topic=\"AI\", question=\"What is deep learning?\")\n",
    "\n",
    "# LLM 호출\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "# 결과 출력\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-farQSE-J-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
