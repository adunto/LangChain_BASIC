{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7df3d014",
   "metadata": {},
   "source": [
    "## Hugging Face Tokenizer\n",
    "### **1. 개요**\n",
    "\n",
    "Hugging Face의 **Tokenizer**는 NLP 모델에서 텍스트를 처리할 때 **토큰화(Tokenization)** 작업을 수행하는 도구입니다.\n",
    "\n",
    "텍스트를 **단어, 서브워드, 문장 등 다양한 단위로 변환**하며, 사전 학습된 토큰화 방식을 활용하여 **언어 모델의 입력 형식에 맞게 변환**합니다.\n",
    "\n",
    "### **2. 주요 특징**\n",
    "\n",
    "- **사전 학습된 모델 사용 가능** → BERT, GPT, T5 등 다양한 모델 지원\n",
    "- **Fast Tokenizer 제공** → Rust 기반의 **빠른 토큰화(`FastTokenizer`)** 지원\n",
    "- **다양한 토큰화 방식 지원** → WordPiece, Byte Pair Encoding(BPE), SentencePiece 등\n",
    "- **특수 토큰 처리 가능** → `[CLS]`, `[SEP]`, `[MASK]`, `<pad>` 등의 토큰을 자동 추가\n",
    "- **병렬 토큰화 지원** → 다중 문장 처리 속도 향상"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7960af9d",
   "metadata": {},
   "source": [
    "### GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70364ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# GPT-2 모델의 토크나이저 로드\n",
    "hf_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "# 데이터 파일 읽기\n",
    "file_path = \"../data/ai-terminology.txt\"\n",
    "with open(file_path, encoding=\"utf-8\") as f:\n",
    "    file_content = f.read()\n",
    "\n",
    "print(\" 원본 텍스트 미리보기:\\n\", file_content[:200])\n",
    "\n",
    "# CharacterTextSplitter 설정 (Hugging Face 토크나이저 사용)\n",
    "text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    hf_tokenizer,\n",
    "    chunk_size=300,  # 각 청크 크기 (토큰 기준 아님)\n",
    "    chunk_overlap=50,  # 청크 간 중복 부분\n",
    ")\n",
    "\n",
    "# 텍스트 분할 수행\n",
    "split_texts = text_splitter.split_text(file_content)\n",
    "\n",
    "# 분할된 텍스트 출력\n",
    "print(f\"\\n 총 {len(split_texts)}개의 청크로 분할됨\\n\")\n",
    "for i, chunk in enumerate(split_texts[:5]):  # 처음 5개만 출력\n",
    "    print(f\" Chunk {i+1} ({len(chunk)}자):\\n{chunk}\\n\")\n",
    "\n",
    "# 토크나이저로 텍스트를 토큰 단위로 변환하여 확인\n",
    "tokenized_example = hf_tokenizer.tokenize(split_texts[0])\n",
    "print(f\"\\n 첫 번째 청크의 토큰 개수: {len(tokenized_example)}\")\n",
    "print(\" 첫 번째 청크의 토큰 리스트:\", tokenized_example[:20])  # 앞 20개만 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f96dc9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 토크나이저 로딩 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5172 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Created a chunk of size 321, which is longer than the specified 300\n",
      "Created a chunk of size 362, which is longer than the specified 300\n",
      "Created a chunk of size 321, which is longer than the specified 100\n",
      "Created a chunk of size 362, which is longer than the specified 100\n",
      "Created a chunk of size 237, which is longer than the specified 100\n",
      "Created a chunk of size 201, which is longer than the specified 100\n",
      "Created a chunk of size 226, which is longer than the specified 100\n",
      "Created a chunk of size 236, which is longer than the specified 100\n",
      "Created a chunk of size 180, which is longer than the specified 100\n",
      "Created a chunk of size 240, which is longer than the specified 100\n",
      "Created a chunk of size 226, which is longer than the specified 100\n",
      "Created a chunk of size 205, which is longer than the specified 100\n",
      "Created a chunk of size 202, which is longer than the specified 100\n",
      "Created a chunk of size 170, which is longer than the specified 100\n",
      "Created a chunk of size 193, which is longer than the specified 100\n",
      "Created a chunk of size 230, which is longer than the specified 100\n",
      "Created a chunk of size 199, which is longer than the specified 100\n",
      "Created a chunk of size 206, which is longer than the specified 100\n",
      "Created a chunk of size 185, which is longer than the specified 100\n",
      "Created a chunk of size 192, which is longer than the specified 100\n",
      "Created a chunk of size 207, which is longer than the specified 100\n",
      "Created a chunk of size 202, which is longer than the specified 100\n",
      "Created a chunk of size 321, which is longer than the specified 300\n",
      "Created a chunk of size 362, which is longer than the specified 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토크나이저 로딩 완료\n",
      "파일 읽기 성공: ../data/ai-terminology.txt\n",
      "\n",
      "전체 텍스트 길이: 3036자\n",
      "\n",
      "============================================================\n",
      "GPT-2 토크나이저 기본 정보\n",
      "============================================================\n",
      "모델명: gpt2\n",
      "어휘 크기: 50,257개\n",
      "최대 입력 길이: 1,024 토큰\n",
      "패딩 토큰: None\n",
      "시작 토큰: <|endoftext|>\n",
      "끝 토큰: <|endoftext|>\n",
      "언노운 토큰: <|endoftext|>\n",
      "\n",
      "원본 텍스트 토큰 개수: 5,172개\n",
      "문자 대 토큰 비율: 0.59 (문자/토큰)\n",
      "\n",
      "============================================================\n",
      "토크나이저 동작 확인\n",
      "============================================================\n",
      "테스트 텍스트: RAG는 검색 기반의 텍스트 생성 모델입니다. Hello, AI!\n",
      "\n",
      "1. 토큰화 결과: ['RAG', 'ë', 'Ĭ', 'Ķ', 'Ġ', 'ê', '²', 'Ģ', 'ì', 'ĥ', 'ī', 'Ġ', 'ê', '¸', '°', 'ë', '°', 'ĺ', 'ìĿ', 'ĺ', 'Ġ', 'í', 'ħ', 'į', 'ì', 'Ĭ', '¤', 'í', 'Ĭ', '¸', 'Ġì', 'ĥ', 'Ŀ', 'ì', 'Ħ', '±', 'Ġë', 'ª', '¨', 'ë', 'į', '¸', 'ì', 'ŀ', 'ħ', 'ëĭ', 'Ī', 'ëĭ', '¤', '.', 'ĠHello', ',', 'ĠAI', '!']\n",
      "   토큰 개수: 54개\n",
      "\n",
      "2. 인코딩 결과: [33202, 167, 232, 242, 220, 166, 110, 222, 168, 225, 231, 220, 166, 116, 108, 167, 108, 246, 35975, 246, 220, 169, 227, 235, 168, 232, 97, 169, 232, 116, 23821, 225, 251, 168, 226, 109, 31619, 103, 101, 167, 235, 116, 168, 252, 227, 46695, 230, 46695, 97, 13, 18435, 11, 9552, 0]\n",
      "   토큰 ID 개수: 54개\n",
      "\n",
      "3. 디코딩 결과: RAG는 검색 기반의 텍스트 생성 모델입니다. Hello, AI!\n",
      "\n",
      "4. 토큰별 ID 매핑:\n",
      "    1. 'RAG' -> 33202\n",
      "    2. 'ë' -> 167\n",
      "    3. 'Ĭ' -> 232\n",
      "    4. 'Ķ' -> 242\n",
      "    5. 'Ġ' -> 220\n",
      "    6. 'ê' -> 166\n",
      "    7. '²' -> 110\n",
      "    8. 'Ģ' -> 222\n",
      "    9. 'ì' -> 168\n",
      "   10. 'ĥ' -> 225\n",
      "   11. 'ī' -> 231\n",
      "   12. 'Ġ' -> 220\n",
      "   13. 'ê' -> 166\n",
      "   14. '¸' -> 116\n",
      "   15. '°' -> 108\n",
      "   16. 'ë' -> 167\n",
      "   17. '°' -> 108\n",
      "   18. 'ĺ' -> 246\n",
      "   19. 'ìĿ' -> 35975\n",
      "   20. 'ĺ' -> 246\n",
      "   21. 'Ġ' -> 220\n",
      "   22. 'í' -> 169\n",
      "   23. 'ħ' -> 227\n",
      "   24. 'į' -> 235\n",
      "   25. 'ì' -> 168\n",
      "   26. 'Ĭ' -> 232\n",
      "   27. '¤' -> 97\n",
      "   28. 'í' -> 169\n",
      "   29. 'Ĭ' -> 232\n",
      "   30. '¸' -> 116\n",
      "   31. 'Ġì' -> 23821\n",
      "   32. 'ĥ' -> 225\n",
      "   33. 'Ŀ' -> 251\n",
      "   34. 'ì' -> 168\n",
      "   35. 'Ħ' -> 226\n",
      "   36. '±' -> 109\n",
      "   37. 'Ġë' -> 31619\n",
      "   38. 'ª' -> 103\n",
      "   39. '¨' -> 101\n",
      "   40. 'ë' -> 167\n",
      "   41. 'į' -> 235\n",
      "   42. '¸' -> 116\n",
      "   43. 'ì' -> 168\n",
      "   44. 'ŀ' -> 252\n",
      "   45. 'ħ' -> 227\n",
      "   46. 'ëĭ' -> 46695\n",
      "   47. 'Ī' -> 230\n",
      "   48. 'ëĭ' -> 46695\n",
      "   49. '¤' -> 97\n",
      "   50. '.' -> 13\n",
      "   51. 'ĠHello' -> 18435\n",
      "   52. ',' -> 11\n",
      "   53. 'ĠAI' -> 9552\n",
      "   54. '!' -> 0\n",
      "\n",
      "============================================================\n",
      "CharacterTextSplitter with HuggingFace 토크나이저\n",
      "============================================================\n",
      "총 23개의 청크로 분할됨\n",
      "\n",
      "처음 3개 청크 분석:\n",
      "\n",
      "Chunk 1:\n",
      "  문자 수: 25자\n",
      "  토큰 수: 23개\n",
      "  토큰 ID 수: 23개\n",
      "  내용 미리보기: Semantic Search (의미론적 검색)...\n",
      "\n",
      "Chunk 2:\n",
      "  문자 수: 157자\n",
      "  토큰 수: 321개\n",
      "  토큰 ID 수: 321개\n",
      "  내용 미리보기: 정의: 사용자의 질의를 단순한 키워드 매칭이 아니라 문맥과 의미를 분석하여 관련 정보를 반환하는 검색 방식.\n",
      "예시: \"우주 탐사\"를 검색하면 \"아폴로 11호\", \"화성 탐사 로버\"...\n",
      "\n",
      "Chunk 3:\n",
      "  문자 수: 37자\n",
      "  토큰 수: 9개\n",
      "  토큰 ID 수: 9개\n",
      "  내용 미리보기: FAISS (Facebook AI Similarity Search)...\n",
      "\n",
      "============================================================\n",
      "다양한 chunk_size 설정 비교\n",
      "============================================================\n",
      "\n",
      "chunk_size=100 토큰:\n",
      "  총 청크 수: 42개\n",
      "  평균 토큰 수: 121.2개\n",
      "  전체 토큰 수: 5,090개\n",
      "  토큰 수 범위: 9~362개\n",
      "\n",
      "chunk_size=300 토큰:\n",
      "  총 청크 수: 23개\n",
      "  평균 토큰 수: 238.0개\n",
      "  전체 토큰 수: 5,475개\n",
      "  토큰 수 범위: 9~362개\n",
      "\n",
      "chunk_size=500 토큰:\n",
      "  총 청크 수: 13개\n",
      "  평균 토큰 수: 413.6개\n",
      "  전체 토큰 수: 5,377개\n",
      "  토큰 수 범위: 276~491개\n",
      "\n",
      "============================================================\n",
      "토크나이저별 비교\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 321, which is longer than the specified 300\n",
      "Created a chunk of size 362, which is longer than the specified 300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "비교 텍스트: RAG는 검색 기반의 텍스트 생성 모델입니다. AI와 머신러닝을 활용합니다.\n",
      "\n",
      "GPT-2 토크나이저:\n",
      "  토큰 수: 81개\n",
      "  토큰: ['RAG', 'ë', 'Ĭ', 'Ķ', 'Ġ', 'ê', '²', 'Ģ', 'ì', 'ĥ', 'ī', 'Ġ', 'ê', '¸', '°', 'ë', '°', 'ĺ', 'ìĿ', 'ĺ', 'Ġ', 'í', 'ħ', 'į', 'ì', 'Ĭ', '¤', 'í', 'Ĭ', '¸', 'Ġì', 'ĥ', 'Ŀ', 'ì', 'Ħ', '±', 'Ġë', 'ª', '¨', 'ë', 'į', '¸', 'ì', 'ŀ', 'ħ', 'ëĭ', 'Ī', 'ëĭ', '¤', '.', 'ĠAI', 'ì', 'Ļ', 'Ģ', 'Ġë', '¨', '¸', 'ì', 'ĭ', 'ł', 'ë', 'Ł', '¬', 'ëĭ', 'Ŀ', 'ìĿ', 'Ħ', 'Ġ', 'í', 'Ļ', 'ľ', 'ì', 'ļ', '©', 'íķ', '©', 'ëĭ', 'Ī', 'ëĭ', '¤', '.']\n",
      "\n",
      "OpenAI 토크나이저 (cl100k_base):\n",
      "  토큰 수: 33개\n",
      "  토큰 ID: [49, 1929, 16969, 86422, 78326, 55216, 39277, 246, 21028, 10997, 45204, 54289, 53017, 55170, 69697, 116, 80052, 13, 15592, 81673, 5251, 101, 116, 83628, 61394, 9019, 251, 18359, 47932, 250, 27797, 61938, 13]\n",
      "\n",
      "토큰 수 차이: 48개\n",
      "\n",
      "============================================================\n",
      "한국어 처리 성능 테스트\n",
      "============================================================\n",
      "한국어 텍스트별 토큰화 결과:\n",
      "\n",
      "1. '안녕하세요'\n",
      "   토큰: ['ì', 'ķ', 'Ī', 'ë', 'ħ', 'ķ', 'íķ', 'ĺ', 'ì', 'Ħ', '¸', 'ì', 'ļ', 'Ķ']\n",
      "   토큰 수: 14개\n",
      "   문자당 토큰: 2.80\n",
      "\n",
      "2. '인공지능과 머신러닝'\n",
      "   토큰: ['ìĿ', '¸', 'ê', '³', 'µ', 'ì', '§', 'Ģ', 'ë', 'Ĭ', '¥', 'ê', '³', '¼', 'Ġë', '¨', '¸', 'ì', 'ĭ', 'ł', 'ë', 'Ł', '¬', 'ëĭ', 'Ŀ']\n",
      "   토큰 수: 25개\n",
      "   문자당 토큰: 2.50\n",
      "\n",
      "3. '자연어 처리 기술'\n",
      "   토큰: ['ì', 'ŀ', 'Ĳ', 'ì', 'Ĺ', '°', 'ì', 'ĸ', '´', 'Ġì', '²', 'ĺ', 'ë', '¦', '¬', 'Ġ', 'ê', '¸', '°', 'ì', 'Ī', 'ł']\n",
      "   토큰 수: 22개\n",
      "   문자당 토큰: 2.44\n",
      "\n",
      "4. '딥러닝 모델 학습'\n",
      "   토큰: ['ë', 'Ķ', '¥', 'ë', 'Ł', '¬', 'ëĭ', 'Ŀ', 'Ġë', 'ª', '¨', 'ë', 'į', '¸', 'Ġ', 'íķ', 'Ļ', 'ì', 'Ĭ', 'µ']\n",
      "   토큰 수: 20개\n",
      "   문자당 토큰: 2.22\n",
      "\n",
      "5. '검색 기반 생성 모델'\n",
      "   토큰: ['ê', '²', 'Ģ', 'ì', 'ĥ', 'ī', 'Ġ', 'ê', '¸', '°', 'ë', '°', 'ĺ', 'Ġì', 'ĥ', 'Ŀ', 'ì', 'Ħ', '±', 'Ġë', 'ª', '¨', 'ë', 'į', '¸']\n",
      "   토큰 수: 25개\n",
      "   문자당 토큰: 2.27\n",
      "\n",
      "============================================================\n",
      "Document 객체와 함께 사용\n",
      "============================================================\n",
      "Document 분할 결과: 23개 청크\n",
      "\n",
      "Document Chunk 1:\n",
      "  토큰 수: 23개\n",
      "  메타데이터: {'source': 'ai-terminology.txt', 'type': 'glossary', 'tokenizer': 'gpt2'}\n",
      "  내용: Semantic Search (의미론적 검색)...\n",
      "\n",
      "Document Chunk 2:\n",
      "  토큰 수: 321개\n",
      "  메타데이터: {'source': 'ai-terminology.txt', 'type': 'glossary', 'tokenizer': 'gpt2'}\n",
      "  내용: 정의: 사용자의 질의를 단순한 키워드 매칭이 아니라 문맥과 의미를 분석하여 관련 정보를 반환하는 검색 방식.\n",
      "예시: \"우주 탐사\"를 검색하면 \"아폴로 11호\", \"화성 탐사 로버\"...\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# GPT-2 모델의 토크나이저 로드\n",
    "print(\"GPT-2 토크나이저 로딩 중...\")\n",
    "hf_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "print(\"토크나이저 로딩 완료\")\n",
    "\n",
    "# 데이터 파일 읽기\n",
    "file_path = \"../data/ai-terminology.txt\"\n",
    "\n",
    "# 파일 읽기\n",
    "try:\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        file_content = f.read()\n",
    "    print(f\"파일 읽기 성공: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"파일 읽기 실패: {e}\")\n",
    "    exit()\n",
    "\n",
    "# print(\"원본 텍스트 미리보기:\")\n",
    "# print(\"-\" * 50)\n",
    "# print(file_content[:200] + \"...\")\n",
    "print(f\"\\n전체 텍스트 길이: {len(file_content)}자\")\n",
    "\n",
    "# ==========================================\n",
    "# GPT-2 토크나이저 기본 정보\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GPT-2 토크나이저 기본 정보\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"모델명: {hf_tokenizer.name_or_path}\")\n",
    "print(f\"어휘 크기: {hf_tokenizer.vocab_size:,}개\")\n",
    "print(f\"최대 입력 길이: {hf_tokenizer.model_max_length:,} 토큰\")\n",
    "\n",
    "# 특수 토큰 확인\n",
    "print(f\"패딩 토큰: {hf_tokenizer.pad_token}\")\n",
    "print(f\"시작 토큰: {hf_tokenizer.bos_token}\")\n",
    "print(f\"끝 토큰: {hf_tokenizer.eos_token}\")\n",
    "print(f\"언노운 토큰: {hf_tokenizer.unk_token}\")\n",
    "\n",
    "# 전체 텍스트의 토큰 개수 확인\n",
    "total_tokens = len(hf_tokenizer.tokenize(file_content))\n",
    "print(f\"\\n원본 텍스트 토큰 개수: {total_tokens:,}개\")\n",
    "print(f\"문자 대 토큰 비율: {len(file_content)/total_tokens:.2f} (문자/토큰)\")\n",
    "\n",
    "# ==========================================\n",
    "# 토크나이저 동작 확인\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"토크나이저 동작 확인\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 예제 텍스트로 토크나이저 테스트\n",
    "test_text = \"RAG는 검색 기반의 텍스트 생성 모델입니다. Hello, AI!\"\n",
    "print(f\"테스트 텍스트: {test_text}\")\n",
    "\n",
    "# 1. 토큰화 (tokenize)\n",
    "tokens = hf_tokenizer.tokenize(test_text)\n",
    "print(f\"\\n1. 토큰화 결과: {tokens}\")\n",
    "print(f\"   토큰 개수: {len(tokens)}개\")\n",
    "\n",
    "# 2. 인코딩 (encode) - 토큰을 ID로 변환\n",
    "token_ids = hf_tokenizer.encode(test_text)\n",
    "print(f\"\\n2. 인코딩 결과: {token_ids}\")\n",
    "print(f\"   토큰 ID 개수: {len(token_ids)}개\")\n",
    "\n",
    "# 3. 디코딩 (decode) - ID를 다시 텍스트로 변환\n",
    "decoded_text = hf_tokenizer.decode(token_ids)\n",
    "print(f\"\\n3. 디코딩 결과: {decoded_text}\")\n",
    "\n",
    "# 4. 개별 토큰 ID 확인\n",
    "print(f\"\\n4. 토큰별 ID 매핑:\")\n",
    "for i, (token, token_id) in enumerate(zip(tokens, token_ids), 1):\n",
    "    print(f\"   {i:2d}. '{token}' -> {token_id}\")\n",
    "\n",
    "# ==========================================\n",
    "# CharacterTextSplitter with HuggingFace 토크나이저\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CharacterTextSplitter with HuggingFace 토크나이저\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# CharacterTextSplitter 설정 (Hugging Face 토크나이저 사용)\n",
    "text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    hf_tokenizer,\n",
    "    chunk_size=300,     # 토큰 개수 기준\n",
    "    chunk_overlap=50,   # 토큰 단위 중복\n",
    "    separator=\"\\n\\n\"    # 구분자 설정\n",
    ")\n",
    "\n",
    "# 텍스트 분할 수행\n",
    "split_texts = text_splitter.split_text(file_content)\n",
    "\n",
    "print(f\"총 {len(split_texts)}개의 청크로 분할됨\")\n",
    "print(\"\\n처음 3개 청크 분석:\")\n",
    "\n",
    "for i, chunk in enumerate(split_texts[:3], 1):\n",
    "    chunk_tokens = hf_tokenizer.tokenize(chunk)\n",
    "    chunk_token_ids = hf_tokenizer.encode(chunk)\n",
    "    \n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(f\"  문자 수: {len(chunk):,}자\")\n",
    "    print(f\"  토큰 수: {len(chunk_tokens):,}개\")\n",
    "    print(f\"  토큰 ID 수: {len(chunk_token_ids):,}개\")\n",
    "    print(f\"  내용 미리보기: {chunk[:100]}...\")\n",
    "\n",
    "# ==========================================\n",
    "# 다양한 chunk_size 비교\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"다양한 chunk_size 설정 비교\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "chunk_sizes = [100, 300, 500]\n",
    "\n",
    "for chunk_size in chunk_sizes:\n",
    "    print(f\"\\nchunk_size={chunk_size} 토큰:\")\n",
    "    \n",
    "    splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        hf_tokenizer,\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_size//10,  # 10% 중복\n",
    "        separator=\"\\n\\n\"\n",
    "    )\n",
    "    \n",
    "    chunks = splitter.split_text(file_content)\n",
    "    \n",
    "    # 통계 계산\n",
    "    total_tokens = sum(len(hf_tokenizer.tokenize(chunk)) for chunk in chunks)\n",
    "    avg_tokens = total_tokens / len(chunks) if chunks else 0\n",
    "    \n",
    "    print(f\"  총 청크 수: {len(chunks)}개\")\n",
    "    print(f\"  평균 토큰 수: {avg_tokens:.1f}개\")\n",
    "    print(f\"  전체 토큰 수: {total_tokens:,}개\")\n",
    "    \n",
    "    # 토큰 수 분포 확인\n",
    "    token_counts = [len(hf_tokenizer.tokenize(chunk)) for chunk in chunks]\n",
    "    min_tokens = min(token_counts) if token_counts else 0\n",
    "    max_tokens = max(token_counts) if token_counts else 0\n",
    "    \n",
    "    print(f\"  토큰 수 범위: {min_tokens}~{max_tokens}개\")\n",
    "\n",
    "# ==========================================\n",
    "# 토크나이저별 비교 (GPT-2 vs tiktoken)\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"토크나이저별 비교\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# tiktoken과 비교 (OpenAI 토크나이저)\n",
    "try:\n",
    "    import tiktoken\n",
    "    \n",
    "    # OpenAI 토크나이저\n",
    "    openai_encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    # 같은 텍스트로 비교\n",
    "    comparison_text = \"RAG는 검색 기반의 텍스트 생성 모델입니다. AI와 머신러닝을 활용합니다.\"\n",
    "    \n",
    "    # GPT-2 토큰화\n",
    "    gpt2_tokens = hf_tokenizer.tokenize(comparison_text)\n",
    "    gpt2_token_count = len(gpt2_tokens)\n",
    "    \n",
    "    # OpenAI 토큰화\n",
    "    openai_tokens = openai_encoding.encode(comparison_text)\n",
    "    openai_token_count = len(openai_tokens)\n",
    "    \n",
    "    print(f\"비교 텍스트: {comparison_text}\")\n",
    "    print(f\"\\nGPT-2 토크나이저:\")\n",
    "    print(f\"  토큰 수: {gpt2_token_count}개\")\n",
    "    print(f\"  토큰: {gpt2_tokens}\")\n",
    "    \n",
    "    print(f\"\\nOpenAI 토크나이저 (cl100k_base):\")\n",
    "    print(f\"  토큰 수: {openai_token_count}개\")\n",
    "    print(f\"  토큰 ID: {openai_tokens}\")\n",
    "    \n",
    "    print(f\"\\n토큰 수 차이: {abs(gpt2_token_count - openai_token_count)}개\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"tiktoken이 설치되지 않음. GPT-2 토크나이저만 사용합니다.\")\n",
    "\n",
    "# ==========================================\n",
    "# 한국어 처리 성능 테스트\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"한국어 처리 성능 테스트\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "korean_texts = [\n",
    "    \"안녕하세요\",\n",
    "    \"인공지능과 머신러닝\",\n",
    "    \"자연어 처리 기술\",\n",
    "    \"딥러닝 모델 학습\",\n",
    "    \"검색 기반 생성 모델\"\n",
    "]\n",
    "\n",
    "print(\"한국어 텍스트별 토큰화 결과:\")\n",
    "for i, text in enumerate(korean_texts, 1):\n",
    "    tokens = hf_tokenizer.tokenize(text)\n",
    "    token_ids = hf_tokenizer.encode(text)\n",
    "    \n",
    "    print(f\"\\n{i}. '{text}'\")\n",
    "    print(f\"   토큰: {tokens}\")\n",
    "    print(f\"   토큰 수: {len(tokens)}개\")\n",
    "    print(f\"   문자당 토큰: {len(tokens)/len(text):.2f}\")\n",
    "\n",
    "# ==========================================\n",
    "# Document 객체와 함께 사용\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Document 객체와 함께 사용\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "# Document 객체 생성\n",
    "document = Document(\n",
    "    page_content=file_content,\n",
    "    metadata={\n",
    "        \"source\": \"ai-terminology.txt\",\n",
    "        \"type\": \"glossary\",\n",
    "        \"tokenizer\": \"gpt2\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Document 분할\n",
    "doc_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    hf_tokenizer,\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "doc_chunks = doc_splitter.split_documents([document])\n",
    "\n",
    "print(f\"Document 분할 결과: {len(doc_chunks)}개 청크\")\n",
    "\n",
    "for i, doc_chunk in enumerate(doc_chunks[:2], 1):\n",
    "    token_count = len(hf_tokenizer.tokenize(doc_chunk.page_content))\n",
    "    print(f\"\\nDocument Chunk {i}:\")\n",
    "    print(f\"  토큰 수: {token_count}개\")\n",
    "    print(f\"  메타데이터: {doc_chunk.metadata}\")\n",
    "    print(f\"  내용: {doc_chunk.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3723ea",
   "metadata": {},
   "source": [
    "### AutoTokenizer (한국어 특화된 모델 - KoBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df9c3702",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sogno\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\langchain-basic-farQSE-J-py3.12\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sogno\\.cache\\huggingface\\hub\\models--beomi--kcbert-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 총 2개 청크로 분할됨\n",
      "\n",
      " Chunk 1: 자연어 처리는 인공지능의 핵심 기술 중 하나입니다. NLP 모델은 의미를 이해하고\n",
      "\n",
      " Chunk 2: 모델은 의미를 이해하고 텍스트를 생성할 수 있습니다.\n",
      "\n",
      " 첫 번째 청크의 토큰 개수: 20\n",
      " 첫 번째 청크의 토큰 리스트: ['자연', '##어', '처리', '##는', '인공', '##지능', '##의', '핵심', '기술', '중', '하나', '##입니다', '.', 'N', '##L', '##P', '모델', '##은', '의미를', '이해하고']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 한국어 지원 토크나이저 사용 (kcbert-base)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"beomi/kcbert-base\")  \n",
    "\n",
    "# 예제 문장 (한글 포함)\n",
    "text = \"자연어 처리는 인공지능의 핵심 기술 중 하나입니다. NLP 모델은 의미를 이해하고 텍스트를 생성할 수 있습니다.\"\n",
    "\n",
    "# 적절한 청크 크기 조정 (너무 작지 않게 설정)\n",
    "splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer=tokenizer, \n",
    "    chunk_size=20,  # 더 큰 청크 크기\n",
    "    chunk_overlap=5  # 문맥 유지를 위한 오버랩\n",
    ")\n",
    "\n",
    "# 텍스트 분할\n",
    "split_texts = splitter.split_text(text)\n",
    "\n",
    "# 실행 결과 출력\n",
    "print(f\" 총 {len(split_texts)}개 청크로 분할됨\\n\")\n",
    "for i, chunk in enumerate(split_texts):\n",
    "    print(f\" Chunk {i+1}: {chunk}\\n\")\n",
    "\n",
    "# 첫 번째 청크의 토큰화 결과\n",
    "tokens = tokenizer.tokenize(split_texts[0])\n",
    "print(f\" 첫 번째 청크의 토큰 개수: {len(tokens)}\")\n",
    "print(\" 첫 번째 청크의 토큰 리스트:\", tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-farQSE-J-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
