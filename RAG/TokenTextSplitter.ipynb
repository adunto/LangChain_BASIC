{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "184538dd",
   "metadata": {},
   "source": [
    "## TokenTextSplitter\n",
    "**토큰 개수를 기준으로 문서를 분할하는 방식**입니다.\n",
    "\n",
    "- LLM은 단어가 아닌 **토큰(Token) 단위**로 입력을 처리하므로, 모델의 입력 제한을 초과하지 않도록 분할하는 것이 중요함.\n",
    "- OpenAI의 GPT 계열 모델처럼 **토큰 단위로 입력을 받는 모델**을 사용할 때는 `TokenTextSplitter`가  유용합니다. ( `TokenTextSplitter`를 활용하면 **입력 토큰 제한을 초과하지 않도록 관리 가능함 )**\n",
    "- TokenTextSplitter는 실제 LLM이 인식하는 토큰 개수로 분할하기 때문에 LLM의 실제 토큰 처리 방식과 더 잘 맞음\n",
    "- 내부적으로 `tiktoken` 라이브러리를 사용하여 **토큰 개수를 세고 분할**함.\n",
    "\n",
    "### **tiktoken이란?**\n",
    "\n",
    "- OpenAI에서 개발한 **토큰화(Tokenization) 라이브러리**\n",
    "- GPT-3, GPT-4 등의 모델이 사용하는 토큰화 방식 제공\n",
    "- 예: `\"ChatGPT is amazing!\"` → `[2023, 345, 8723, 12]` 형태의 토큰으로 변환\n",
    "\n",
    "**1. TokenTextSplitter의 특징**\n",
    "\n",
    "- **토큰 단위 분할**: 문자가 아닌 토큰 개수로 청크 크기 결정\n",
    "- **정확한 제어**: LLM 토큰 제한을 정확히 준수\n",
    "- **tiktoken 활용**: OpenAI 모델과 동일한 토큰화 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dbdcc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using version ^0.9.0 for tiktoken\n",
      "\n",
      "Updating dependencies\n",
      "Resolving dependencies...\n",
      "\n",
      "No dependencies to install or update\n",
      "\n",
      "Writing lock file\n"
     ]
    }
   ],
   "source": [
    "!poetry add tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1825ae25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 텍스트 미리보기:\n",
      " Semantic Search (의미론적 검색)\n",
      "\n",
      "정의: 사용자의 질의를 단순한 키워드 매칭이 아니라 문맥과 의미를 분석하여 관련 정보를 반환하는 검색 방식.\n",
      "예시: \"우주 탐사\"를 검색하면 \"아폴로 11호\", \"화성 탐사 로버\"와 같은 연관 정보가 포함된 결과를 제공함.\n",
      "연관 키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "FAISS (Facebook AI Similarity Search)\n",
      "\n",
      "정의: FAISS는 페이스북에서 개발한 고속 유사성 검색 라이브러리로, 특히 대규모 벡터 집합에서 유사 벡터를 효과적으로 검색할 수 있도록 설계되었습니다.\n",
      "예시: 수백만 개의 이미지 벡터 중에서 비슷한 이미지를 빠르게 찾는 데 FAISS가 사용될 수 있습니다.\n",
      "연관키워드: 벡터 검색, 머신러닝, 데이터베이스 최적화\n",
      "\n",
      "Embedding (임베딩)\n",
      "\n",
      "정의: 단어나 문장을 벡터 공간에 매핑하여 의미적으로 유사한 것들이 가까이 위치하도록 하는 기법.\n",
      "예시: \"강아지\"와 \"고양이\"의 벡터 표현이 \n",
      "\n",
      " 총 14개의 청크로 분할됨.\n",
      "\n",
      " 첫 번째 청크:\n",
      " Semantic Search (의미론적 검색)\n",
      "\n",
      "정의: 사용자의 질의를 단순한 키워드 매칭이 아니라 문맥과 의미를 분석하여 관련 정보를 반환하는 검색 방식.\n",
      "예시: \"우주 탐사\"를 검색하면 \"아폴로 11호\", \"화성 탐사 로버\"와 같은 연관 정보가 포함된 결과를 제공함.\n",
      "연관 키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "FAISS (Facebook AI Similarity Search)\n",
      "\n",
      "정의: FAISS는 페이스북에서 개발한 고속 유사성 검색 라이브러리로, 특히 대�\n",
      "\n",
      " Chunk 1 (길이: 270):\n",
      "Semantic Search (의미론적 검색)\n",
      "\n",
      "정의: 사용자의 질의를 단순한 키워드 매칭이 아니라 문맥과 의미를 분석하여 관련 정보를 반환하는 검색 방식.\n",
      "예시: \"우주 탐사\"를 검색하면 \"아폴로 11호\", \"화성 탐사 로버\"와 같은 연관 정보가 포함된 결과를 제공함.\n",
      "연관 키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "FAISS (Facebook AI Similarity Search)\n",
      "\n",
      "정의: FAISS는 페이스북에서 개발한 고속 유사성 검색 라이브러리로, 특히 대�\n",
      "\n",
      " Chunk 2 (길이: 229):\n",
      "성 검색 라이브러리로, 특히 대규모 벡터 집합에서 유사 벡터를 효과적으로 검색할 수 있도록 설계되었습니다.\n",
      "예시: 수백만 개의 이미지 벡터 중에서 비슷한 이미지를 빠르게 찾는 데 FAISS가 사용될 수 있습니다.\n",
      "연관키워드: 벡터 검색, 머신러닝, 데이터베이스 최적화\n",
      "\n",
      "Embedding (임베딩)\n",
      "\n",
      "정의: 단어나 문장을 벡터 공간에 매핑하여 의미적으로 유사한 것들이 가까이 위치하도록 하는 기법.\n",
      "예시: \"강\n",
      "\n",
      " Chunk 3 (길이: 233):\n",
      "까이 위치하도록 하는 기법.\n",
      "예시: \"강아지\"와 \"고양이\"의 벡터 표현이 유사하게 위치함.\n",
      "연관 키워드: 벡터화, 자연어 처리, 딥러닝\n",
      "\n",
      "Token (토큰)\n",
      "\n",
      "정의: 텍스트 데이터를 더 작은 단위(단어, 문자, 문장 등)로 나누는 과정.\n",
      "예시: \"AI는 혁신적이다\"를 [\"AI\", \"는\", \"혁신적\", \"이다\"]로 분할.\n",
      "연관 키워드: 토큰화, NLP, 텍스트 전처리\n",
      "\n",
      "Transformer (트랜스포머)\n",
      "\n",
      "정의: 자\n",
      "\n",
      " Chunk 4 (길이: 237):\n",
      "�리\n",
      "\n",
      "Transformer (트랜스포머)\n",
      "\n",
      "정의: 자연어 처리에서 사용되는 신경망 아키텍처로, 병렬 연산과 장기 의존성 처리가 강점.\n",
      "예시: GPT, BERT 등의 모델이 트랜스포머 기반으로 동작함.\n",
      "연관 키워드: 딥러닝, 자기 주의 메커니즘, NLP\n",
      "\n",
      "Self-Attention (자기 주의 메커니즘)\n",
      "\n",
      "정의: 문장의 모든 단어가 서로에게 가중치를 부여하여 문맥을 이해하는 방식.\n",
      "예시: \"나는 강아지를 좋아한다\"에서 \"\n",
      "\n",
      " Chunk 5 (길이: 237):\n",
      ".\n",
      "예시: \"나는 강아지를 좋아한다\"에서 \"나는\"과 \"좋아한다\"가 강한 연관성을 가짐.\n",
      "연관 키워드: 트랜스포머, BERT, 문맥 학습\n",
      "\n",
      "Fine-Tuning (미세 조정)\n",
      "\n",
      "정의: 사전 학습된 모델을 특정 작업에 맞게 추가 학습하는 과정.\n",
      "예시: GPT 모델을 법률 문서 요약에 맞게 학습.\n",
      "연관 키워드: 전이 학습, 모델 최적화, AI 응용\n",
      "\n",
      "Zero-shot Learning (제로샷 학습)\n",
      "\n",
      "정의: 특정 태스크에 대한\n"
     ]
    }
   ],
   "source": [
    "# 간단 예제\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "# 파일 읽기\n",
    "with open(\"../data/ai-terminology.txt\", encoding=\"utf-8\") as f:\n",
    "    file = f.read()  # 파일 내용을 읽어오기\n",
    "\n",
    "print(\"원본 텍스트 미리보기:\\n\", file[:500])  # 앞 500자 출력\n",
    "\n",
    "# TokenTextSplitter 설정\n",
    "text_splitter = TokenTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=200,  # 청크 크기\n",
    "    chunk_overlap=20,  # 청크 간 겹치는 부분 추가하여 문맥 유지\n",
    "    encoding_name=\"cl100k_base\",  # OpenAI tiktoken 기본 인코딩 사용 (한글 처리 개선)\n",
    "    add_start_index=True  # 각 청크의 시작 인덱스 반환\n",
    ")\n",
    "\n",
    "# 텍스트 분할 실행\n",
    "texts = text_splitter.split_text(file)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"\\n 총 {len(texts)}개의 청크로 분할됨.\")\n",
    "print(\"\\n 첫 번째 청크:\\n\", texts[0])\n",
    "\n",
    "# 청크 길이 확인\n",
    "for i, chunk in enumerate(texts[:5]):  # 처음 5개만 확인\n",
    "    print(f\"\\n Chunk {i+1} (길이: {len(chunk)}):\\n{chunk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55abb6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일 읽기 성공: ../data/ai-terminology.txt\n",
      "원본 텍스트 미리보기:\n",
      "--------------------------------------------------\n",
      "Semantic Search (의미론적 검색)\n",
      "\n",
      "정의: 사용자의 질의를 단순한 키워드 매칭이 아니라 문맥과 의미를 분석하여 관련 정보를 반환하는 검색 방식.\n",
      "예시: \"우주 탐사\"를 검색하면 \"아폴로 11호\", \"화성 탐사 로버\"와 같은 연관 정보가 포함된 결과를 제공함.\n",
      "연관 키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "FAISS (Facebook AI Similarity Search)\n",
      "\n",
      "정의: FAISS는 페이스북에서 개발한 고속 유사성 검색 라이브러리로, 특히 대규모 벡터 집합에서 유사 벡터를 효과적으로 검색할 수 있도록 설계되었습니다.\n",
      "예시: 수백만 개의 이미지 벡터 중에서 비슷한 이미지를 빠르게 찾는 데 FAISS가 사용될 수 있습니다.\n",
      "연관키워드: 벡터 검색, 머신러닝, 데이터베이스 최적화\n",
      "\n",
      "Embedding (임베딩)\n",
      "\n",
      "정의: 단어나 문장을 벡터 공간에 매핑하여 의미적으로 유사한 것들이 가까이 위치하도록 하는 기법.\n",
      "예시: \"강아지\"와 \"고양이\"의 벡터 표현이 ...\n",
      "\n",
      "전체 텍스트 길이: 3036자\n",
      "\n",
      "============================================================\n",
      "TokenTextSplitter 특징 및 다른 Splitter와 비교\n",
      "============================================================\n",
      "\n",
      "원본 텍스트의 토큰 개수: 2407개\n",
      "문자 대 토큰 비율: 1.26 (문자/토큰)\n",
      "\n",
      "1. TokenTextSplitter (토큰 기반 분할):\n",
      "----------------------------------------\n",
      "총 14개 청크 생성\n",
      "\n",
      "Chunk 1:\n",
      "  토큰 수: 200개\n",
      "  문자 수: 270자\n",
      "  내용: Semantic Search (의미론적 검색)\n",
      "\n",
      "정의: 사용자의 질의를 단순한 키워드 매칭이 아니라 문맥과 의미를 분석하여 관련 정보를 반환하는 검색 방식.\n",
      "예시: \"우주 탐사\"를...\n",
      "\n",
      "Chunk 2:\n",
      "  토큰 수: 200개\n",
      "  문자 수: 229자\n",
      "  내용: 성 검색 라이브러리로, 특히 대규모 벡터 집합에서 유사 벡터를 효과적으로 검색할 수 있도록 설계되었습니다.\n",
      "예시: 수백만 개의 이미지 벡터 중에서 비슷한 이미지를 빠르게 찾는 데 ...\n",
      "\n",
      "Chunk 3:\n",
      "  토큰 수: 200개\n",
      "  문자 수: 233자\n",
      "  내용: 까이 위치하도록 하는 기법.\n",
      "예시: \"강아지\"와 \"고양이\"의 벡터 표현이 유사하게 위치함.\n",
      "연관 키워드: 벡터화, 자연어 처리, 딥러닝\n",
      "\n",
      "Token (토큰)\n",
      "\n",
      "정의: 텍스트 데이터...\n",
      "\n",
      "2. RecursiveCharacterTextSplitter (문자 기반 분할):\n",
      "---------------------------------------------\n",
      "총 5개 청크 생성\n",
      "\n",
      "Chunk 1:\n",
      "  토큰 수: 556개\n",
      "  문자 수: 685자\n",
      "  내용: Semantic Search (의미론적 검색)\n",
      "\n",
      "정의: 사용자의 질의를 단순한 키워드 매칭이 아니라 문맥과 의미를 분석하여 관련 정보를 반환하는 검색 방식.\n",
      "예시: \"우주 탐사\"를...\n",
      "\n",
      "Chunk 2:\n",
      "  토큰 수: 608개\n",
      "  문자 수: 730자\n",
      "  내용: Transformer (트랜스포머)\n",
      "\n",
      "정의: 자연어 처리에서 사용되는 신경망 아키텍처로, 병렬 연산과 장기 의존성 처리가 강점.\n",
      "예시: GPT, BERT 등의 모델이 트랜스포머 기...\n",
      "\n",
      "Chunk 3:\n",
      "  토큰 수: 621개\n",
      "  문자 수: 794자\n",
      "  내용: Reinforcement Learning (강화 학습)\n",
      "\n",
      "정의: 보상을 극대화하는 방향으로 행동을 학습하는 AI 기법.\n",
      "예시: 알파고가 바둑에서 최적의 수를 찾기 위해 강화 학습을...\n",
      "\n",
      "============================================================\n",
      "토큰 기반 분할의 장점\n",
      "============================================================\n",
      "\n",
      "LLM 토큰 제한: 200개 토큰\n",
      "\n",
      "토큰 기반 분할 결과:\n",
      "  Chunk 1: 200개 토큰 OK\n",
      "  Chunk 2: 200개 토큰 OK\n",
      "  Chunk 3: 200개 토큰 OK\n",
      "  Chunk 4: 200개 토큰 OK\n",
      "  Chunk 5: 200개 토큰 OK\n",
      "  Chunk 6: 200개 토큰 OK\n",
      "  Chunk 7: 200개 토큰 OK\n",
      "  Chunk 8: 200개 토큰 OK\n",
      "  Chunk 9: 200개 토큰 OK\n",
      "  Chunk 10: 200개 토큰 OK\n",
      "  Chunk 11: 200개 토큰 OK\n",
      "  Chunk 12: 200개 토큰 OK\n",
      "  Chunk 13: 200개 토큰 OK\n",
      "  Chunk 14: 67개 토큰 OK\n",
      "\n",
      "토큰 제한 초과 청크: 0개\n",
      "\n",
      "문자 기반 분할 결과:\n",
      "  Chunk 1: 556개 토큰 초과\n",
      "  Chunk 2: 608개 토큰 초과\n",
      "  Chunk 3: 621개 토큰 초과\n",
      "  Chunk 4: 522개 토큰 초과\n",
      "  Chunk 5: 140개 토큰 OK\n",
      "\n",
      "토큰 제한 초과 청크: 4개\n",
      "\n",
      "============================================================\n",
      "다양한 인코딩 방식 비교\n",
      "============================================================\n",
      "\n",
      "cl100k_base (GPT-4, GPT-3.5-turbo):\n",
      "  토큰 수: 37개\n",
      "  토큰 예시: [31495, 230, 75265, 243, 92245, 0, 15592, 81673, 5251, 101]...\n",
      "\n",
      "p50k_base (GPT-3 (davinci)):\n",
      "  토큰 수: 69개\n",
      "  토큰 예시: [168, 243, 230, 167, 227, 243, 47991, 246, 168, 226]...\n",
      "\n",
      "r50k_base (GPT-3 (ada, babbage, curie)):\n",
      "  토큰 수: 69개\n",
      "  토큰 예시: [168, 243, 230, 167, 227, 243, 47991, 246, 168, 226]...\n",
      "\n",
      "============================================================\n",
      "실무 활용 예제\n",
      "============================================================\n",
      "\n",
      "chunk_size=100 토큰:\n",
      "  총 청크 수: 27개\n",
      "  평균 토큰 수: 98.8개\n",
      "  첫 번째 청크 토큰 수: 100개\n",
      "\n",
      "chunk_size=200 토큰:\n",
      "  총 청크 수: 14개\n",
      "  평균 토큰 수: 190.5개\n",
      "  첫 번째 청크 토큰 수: 200개\n",
      "\n",
      "chunk_size=500 토큰:\n",
      "  총 청크 수: 6개\n",
      "  평균 토큰 수: 442.8개\n",
      "  첫 번째 청크 토큰 수: 500개\n",
      "\n",
      "============================================================\n",
      "메타데이터 포함 Document 분할\n",
      "============================================================\n",
      "Document 분할 결과: 14개 청크\n",
      "\n",
      "Document Chunk 1:\n",
      "  토큰 수: 200개\n",
      "  메타데이터: {'source': 'ai-terminology.txt', 'type': 'glossary', 'start_index': -1}\n",
      "  내용: Semantic Search (의미론적 검색)\n",
      "\n",
      "정의: 사용자의 질의를 단순한 키워드 매칭이 아니라 문맥과 의미를 분석하여 관련 정보를 반환하는 검색 방식.\n",
      "예시: \"우주 탐사\"를...\n",
      "\n",
      "Document Chunk 2:\n",
      "  토큰 수: 200개\n",
      "  메타데이터: {'source': 'ai-terminology.txt', 'type': 'glossary', 'start_index': 252}\n",
      "  내용: 성 검색 라이브러리로, 특히 대규모 벡터 집합에서 유사 벡터를 효과적으로 검색할 수 있도록 설계되었습니다.\n",
      "예시: 수백만 개의 이미지 벡터 중에서 비슷한 이미지를 빠르게 찾는 데 ...\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import tiktoken\n",
    "\n",
    "# 파일 읽기\n",
    "file_path = \"../data/ai-terminology.txt\"\n",
    "\n",
    "# 파일 읽기\n",
    "try:\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        ai_terminology_text = f.read()\n",
    "    print(f\"파일 읽기 성공: {file_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"파일 읽기 실패: {e}\")\n",
    "    exit()\n",
    "\n",
    "print(\"원본 텍스트 미리보기:\")\n",
    "print(\"-\" * 50)\n",
    "print(ai_terminology_text[:500] + \"...\")\n",
    "print(f\"\\n전체 텍스트 길이: {len(ai_terminology_text)}자\")\n",
    "\n",
    "# ==========================================\n",
    "# TokenTextSplitter vs 다른 Splitter 비교\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TokenTextSplitter 특징 및 다른 Splitter와 비교\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. 토큰 개수 확인 (tiktoken 사용)\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "tokens = encoding.encode(ai_terminology_text)\n",
    "print(f\"\\n원본 텍스트의 토큰 개수: {len(tokens)}개\")\n",
    "print(f\"문자 대 토큰 비율: {len(ai_terminology_text)/len(tokens):.2f} (문자/토큰)\")\n",
    "\n",
    "# 2. TokenTextSplitter 설정 및 실행\n",
    "print(\"\\n1. TokenTextSplitter (토큰 기반 분할):\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "token_splitter = TokenTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=200,              # 토큰 개수로 크기 지정\n",
    "    chunk_overlap=20,            # 토큰 단위 중복\n",
    "    encoding_name=\"cl100k_base\", # OpenAI GPT 모델용 인코딩\n",
    "    add_start_index=True         # 시작 인덱스 정보 포함\n",
    ")\n",
    "\n",
    "token_chunks = token_splitter.split_text(ai_terminology_text)\n",
    "print(f\"총 {len(token_chunks)}개 청크 생성\")\n",
    "\n",
    "for i, chunk in enumerate(token_chunks[:3], 1):\n",
    "    chunk_tokens = encoding.encode(chunk)\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(f\"  토큰 수: {len(chunk_tokens)}개\")\n",
    "    print(f\"  문자 수: {len(chunk)}자\")\n",
    "    print(f\"  내용: {chunk[:100]}...\")\n",
    "\n",
    "# 3. 다른 Splitter와 비교\n",
    "print(\"\\n2. RecursiveCharacterTextSplitter (문자 기반 분할):\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "char_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,  # 문자 개수로 크기 지정 (토큰 200개 ≈ 문자 800개)\n",
    "    chunk_overlap=80,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    ")\n",
    "\n",
    "char_chunks = char_splitter.split_text(ai_terminology_text)\n",
    "print(f\"총 {len(char_chunks)}개 청크 생성\")\n",
    "\n",
    "for i, chunk in enumerate(char_chunks[:3], 1):\n",
    "    chunk_tokens = encoding.encode(chunk)\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(f\"  토큰 수: {len(chunk_tokens)}개\")\n",
    "    print(f\"  문자 수: {len(chunk)}자\")\n",
    "    print(f\"  내용: {chunk[:100]}...\")\n",
    "\n",
    "# ==========================================\n",
    "# 토큰 기반 분할의 장점 확인\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"토큰 기반 분할의 장점\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# LLM 토큰 제한 시뮬레이션\n",
    "max_tokens = 200  # 가상의 토큰 제한\n",
    "\n",
    "print(f\"\\nLLM 토큰 제한: {max_tokens}개 토큰\")\n",
    "print(\"\\n토큰 기반 분할 결과:\")\n",
    "over_limit_count = 0\n",
    "for i, chunk in enumerate(token_chunks, 1):\n",
    "    chunk_tokens = len(encoding.encode(chunk))\n",
    "    status = \"OK\" if chunk_tokens <= max_tokens else \"초과\"\n",
    "    if chunk_tokens > max_tokens:\n",
    "        over_limit_count += 1\n",
    "    print(f\"  Chunk {i}: {chunk_tokens}개 토큰 {status}\")\n",
    "\n",
    "print(f\"\\n토큰 제한 초과 청크: {over_limit_count}개\")\n",
    "\n",
    "print(\"\\n문자 기반 분할 결과:\")\n",
    "over_limit_count = 0\n",
    "for i, chunk in enumerate(char_chunks, 1):\n",
    "    chunk_tokens = len(encoding.encode(chunk))\n",
    "    status = \"OK\" if chunk_tokens <= max_tokens else \"초과\"\n",
    "    if chunk_tokens > max_tokens:\n",
    "        over_limit_count += 1\n",
    "    print(f\"  Chunk {i}: {chunk_tokens}개 토큰 {status}\")\n",
    "\n",
    "print(f\"\\n토큰 제한 초과 청크: {over_limit_count}개\")\n",
    "\n",
    "# ==========================================\n",
    "# 다양한 encoding 테스트\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"다양한 인코딩 방식 비교\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "encodings = [\n",
    "    (\"cl100k_base\", \"GPT-4, GPT-3.5-turbo\"),\n",
    "    (\"p50k_base\", \"GPT-3 (davinci)\"),\n",
    "    (\"r50k_base\", \"GPT-3 (ada, babbage, curie)\")\n",
    "]\n",
    "\n",
    "test_text = \"안녕하세요! AI와 머신러닝에 대해 배워보겠습니다. Hello, let's learn about AI and Machine Learning!\"\n",
    "\n",
    "for encoding_name, description in encodings:\n",
    "    enc = tiktoken.get_encoding(encoding_name)\n",
    "    tokens = enc.encode(test_text)\n",
    "    print(f\"\\n{encoding_name} ({description}):\")\n",
    "    print(f\"  토큰 수: {len(tokens)}개\")\n",
    "    print(f\"  토큰 예시: {tokens[:10]}...\")\n",
    "\n",
    "# ==========================================\n",
    "# 활용 예제\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"실무 활용 예제\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 다양한 chunk_size로 테스트\n",
    "chunk_sizes = [100, 200, 500]\n",
    "\n",
    "for chunk_size in chunk_sizes:\n",
    "    print(f\"\\nchunk_size={chunk_size} 토큰:\")\n",
    "    splitter = TokenTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_size//10,  # 10% 중복\n",
    "        encoding_name=\"cl100k_base\"\n",
    "    )\n",
    "    \n",
    "    chunks = splitter.split_text(ai_terminology_text)\n",
    "    avg_tokens = sum(len(encoding.encode(chunk)) for chunk in chunks) / len(chunks)\n",
    "    \n",
    "    print(f\"  총 청크 수: {len(chunks)}개\")\n",
    "    print(f\"  평균 토큰 수: {avg_tokens:.1f}개\")\n",
    "    print(f\"  첫 번째 청크 토큰 수: {len(encoding.encode(chunks[0]))}개\")\n",
    "\n",
    "# ==========================================\n",
    "# 메타데이터 포함 문서 분할\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"메타데이터 포함 Document 분할\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Document 객체 생성\n",
    "document = Document(\n",
    "    page_content=ai_terminology_text,\n",
    "    metadata={\"source\": \"ai-terminology.txt\", \"type\": \"glossary\"}\n",
    ")\n",
    "\n",
    "# Document 분할\n",
    "doc_chunks = token_splitter.split_documents([document])\n",
    "\n",
    "print(f\"Document 분할 결과: {len(doc_chunks)}개 청크\")\n",
    "for i, doc_chunk in enumerate(doc_chunks[:2], 1):\n",
    "    tokens_count = len(encoding.encode(doc_chunk.page_content))\n",
    "    print(f\"\\nDocument Chunk {i}:\")\n",
    "    print(f\"  토큰 수: {tokens_count}개\")\n",
    "    print(f\"  메타데이터: {doc_chunk.metadata}\")\n",
    "    print(f\"  내용: {doc_chunk.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98d3368c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aiofiles                 24.1.0      File support for asyncio.\n",
      "aiohappyeyeballs         2.6.1       Happy Eyeballs for asyncio\n",
      "aiohttp                  3.12.12     Async http client/server framework (as...\n",
      "aiosignal                1.3.2       aiosignal: a list of registered asynch...\n",
      "annotated-types          0.7.0       Reusable constraint types to use with ...\n",
      "anyio                    4.9.0       High level compatibility layer for mul...\n",
      "asttokens                3.0.0       Annotate AST trees with source code po...\n",
      "attrs                    25.3.0      Classes Without Boilerplate\n",
      "certifi                  2025.4.26   Python package for providing Mozilla's...\n",
      "charset-normalizer       3.4.2       The Real First Universal Charset Detec...\n",
      "click                    8.2.1       Composable command line interface toolkit\n",
      "colorama                 0.4.6       Cross-platform colored terminal text.\n",
      "comm                     0.2.2       Jupyter Python Comm implementation, fo...\n",
      "dataclasses-json         0.6.7       Easily serialize dataclasses to and fr...\n",
      "debugpy                  1.8.14      An implementation of the Debug Adapter...\n",
      "decorator                5.2.1       Decorators for Humans\n",
      "distro                   1.9.0       Distro - an OS platform information API\n",
      "executing                2.2.0       Get the currently executing AST node o...\n",
      "faiss-cpu                1.11.0      A library for efficient similarity sea...\n",
      "fastapi                  0.115.12    FastAPI framework, high performance, e...\n",
      "ffmpy                    0.6.0       A simple Python wrapper for FFmpeg\n",
      "filelock                 3.18.0      A platform independent file lock.\n",
      "frozenlist               1.7.0       A list-like structure which implements...\n",
      "fsspec                   2025.5.1    File-system specification\n",
      "gradio                   5.33.1      Python library for easily interacting ...\n",
      "gradio-client            1.10.3      Python library for easily interacting ...\n",
      "greenlet                 3.2.3       Lightweight in-process concurrent prog...\n",
      "groovy                   0.1.2       A small Python library created to help...\n",
      "h11                      0.16.0      A pure-Python, bring-your-own-I/O impl...\n",
      "httpcore                 1.0.9       A minimal low-level HTTP client.\n",
      "httpx                    0.28.1      The next generation HTTP client.\n",
      "httpx-sse                0.4.0       Consume Server-Sent Event (SSE) messag...\n",
      "huggingface-hub          0.32.4      Client library to download and publish...\n",
      "idna                     3.10        Internationalized Domain Names in Appl...\n",
      "ipykernel                6.29.5      IPython Kernel for Jupyter\n",
      "ipython                  9.3.0       IPython: Productive Interactive Computing\n",
      "ipython-pygments-lexers  1.1.1       Defines a variety of Pygments lexers f...\n",
      "jedi                     0.19.2      An autocompletion tool for Python that...\n",
      "jinja2                   3.1.6       A very fast and expressive template en...\n",
      "jiter                    0.10.0      Fast iterable JSON parser.\n",
      "jsonpatch                1.33        Apply JSON-Patches (RFC 6902)\n",
      "jsonpointer              3.0.0       Identify specific nodes in a JSON docu...\n",
      "jupyter-client           8.6.3       Jupyter protocol implementation and cl...\n",
      "jupyter-core             5.8.1       Jupyter core package. A base package o...\n",
      "langchain                0.3.25      Building applications with LLMs throug...\n",
      "langchain-community      0.3.25      Community contributed LangChain integr...\n",
      "langchain-core           0.3.65      Building applications with LLMs throug...\n",
      "langchain-openai         0.3.21      An integration package connecting Open...\n",
      "langchain-text-splitters 0.3.8       LangChain text splitting utilities\n",
      "langsmith                0.3.45      Client library to connect to the LangS...\n",
      "markdown-it-py           3.0.0       Python port of markdown-it. Markdown p...\n",
      "markupsafe               3.0.2       Safely add untrusted strings to HTML/X...\n",
      "marshmallow              3.26.1      A lightweight library for converting c...\n",
      "matplotlib-inline        0.1.7       Inline Matplotlib backend for Jupyter\n",
      "mdurl                    0.1.2       Markdown URL utilities\n",
      "multidict                6.4.4       multidict implementation\n",
      "mypy-extensions          1.1.0       Type system extensions for programs ch...\n",
      "nest-asyncio             1.6.0       Patch asyncio to allow nested event loops\n",
      "numpy                    2.3.0       Fundamental package for array computin...\n",
      "openai                   1.85.0      The official Python library for the op...\n",
      "orjson                   3.10.18     Fast, correct Python JSON library supp...\n",
      "packaging                24.2        Core utilities for Python packages\n",
      "pandas                   2.3.0       Powerful data structures for data anal...\n",
      "parso                    0.8.4       A Python Parser\n",
      "pillow                   11.2.1      Python Imaging Library (Fork)\n",
      "platformdirs             4.3.8       A small Python package for determining...\n",
      "prompt-toolkit           3.0.51      Library for building powerful interact...\n",
      "propcache                0.3.2       Accelerated property cache\n",
      "psutil                   7.0.0       Cross-platform lib for process and sys...\n",
      "pure-eval                0.2.3       Safely evaluate AST nodes without side...\n",
      "pydantic                 2.11.5      Data validation using Python type hints\n",
      "pydantic-core            2.33.2      Core functionality for Pydantic valida...\n",
      "pydantic-settings        2.9.1       Settings management using Pydantic\n",
      "pydub                    0.25.1      Manipulate audio with an simple and ea...\n",
      "pygments                 2.19.1      Pygments is a syntax highlighting pack...\n",
      "pypdf                    5.6.0       A pure-python PDF library capable of s...\n",
      "python-dateutil          2.9.0.post0 Extensions to the standard Python date...\n",
      "python-dotenv            1.1.0       Read key-value pairs from a .env file ...\n",
      "python-multipart         0.0.20      A streaming multipart parser for Python\n",
      "pytz                     2025.2      World timezone definitions, modern and...\n",
      "pywin32                  310         Python for Window Extensions\n",
      "pyyaml                   6.0.2       YAML parser and emitter for Python\n",
      "pyzmq                    26.4.0      Python bindings for 0MQ\n",
      "regex                    2024.11.6   Alternative regular expression module,...\n",
      "requests                 2.32.4      Python HTTP for Humans.\n",
      "requests-toolbelt        1.0.0       A utility belt for advanced users of p...\n",
      "rich                     14.0.0      Render rich text, tables, progress bar...\n",
      "ruff                     0.11.13     An extremely fast Python linter and co...\n",
      "safehttpx                0.1.6       A small Python library created to help...\n",
      "semantic-version         2.10.0      A library implementing the 'SemVer' sc...\n",
      "shellingham              1.5.4       Tool to Detect Surrounding Shell\n",
      "six                      1.17.0      Python 2 and 3 compatibility utilities\n",
      "sniffio                  1.3.1       Sniff out which async library your cod...\n",
      "sqlalchemy               2.0.41      Database Abstraction Library\n",
      "stack-data               0.6.3       Extract data from python stack frames ...\n",
      "starlette                0.46.2      The little ASGI library that shines.\n",
      "tenacity                 9.1.2       Retry code until it succeeds\n",
      "tiktoken                 0.9.0       tiktoken is a fast BPE tokeniser for u...\n",
      "tomlkit                  0.13.3      Style preserving TOML library\n",
      "tornado                  6.5.1       Tornado is a Python web framework and ...\n",
      "tqdm                     4.67.1      Fast, Extensible Progress Meter\n",
      "traitlets                5.14.3      Traitlets Python configuration system\n",
      "typer                    0.16.0      Typer, build great CLIs. Easy to code....\n",
      "typing-extensions        4.14.0      Backported and Experimental Type Hints...\n",
      "typing-inspect           0.9.0       Runtime inspection utilities for typin...\n",
      "typing-inspection        0.4.1       Runtime typing introspection tools\n",
      "tzdata                   2025.2      Provider of IANA time zone data\n",
      "urllib3                  2.4.0       HTTP library with thread-safe connecti...\n",
      "uvicorn                  0.34.3      The lightning-fast ASGI server.\n",
      "wcwidth                  0.2.13      Measures the displayed width of unicod...\n",
      "websockets               15.0.1      An implementation of the WebSocket Pro...\n",
      "yarl                     1.20.1      Yet another URL library\n",
      "zstandard                0.23.0      Zstandard bindings for Python\n"
     ]
    }
   ],
   "source": [
    "!poetry show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-farQSE-J-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
