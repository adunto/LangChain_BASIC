{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3009029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(\"http://127.0.0.1:11434\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ebdac3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                ID              SIZE      MODIFIED       \n",
      "qwen2.5:1.5b        65ec06548149    986 MB    21 minutes ago    \n",
      "deepseek-r1:1.5b    e0979632db5a    1.1 GB    30 minutes ago    \n"
     ]
    }
   ],
   "source": [
    "!ollama ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a4b56f",
   "metadata": {},
   "source": [
    "## deepseek 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62086ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96aaa729",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOllama(model=\"deepseek-r1:1.5b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc009996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 더 정확한 응답을 위한 개선된 프롬프트\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that provides accurate and detailed answers.\"),\n",
    "    (\"human\", \"Q: {question}\\nA:\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9457ca27",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65a24cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "('<think>\\n'\n",
      " \"Okay, so I'm trying to understand what LangChain is. From the previous \"\n",
      " \"response, it seems like it's a framework for working with language models in \"\n",
      " 'Python. But I want to make sure I really grasp how this works and why '\n",
      " 'someone would use it instead of other approaches.\\n'\n",
      " '\\n'\n",
      " 'First off, I know that in programming, especially when dealing with AI or '\n",
      " 'NLP tasks, we often use different tools and libraries. For example, PyTorch '\n",
      " 'is a deep learning framework used for neural networks. LangChain must be '\n",
      " 'similar but perhaps more focused on language models.\\n'\n",
      " '\\n'\n",
      " \"The user mentioned LangChain's components: a base model class, data loading \"\n",
      " 'utilities, interfaces for interacting with models, and the main module for '\n",
      " 'handling everything. It also talks about using it in projects with specific '\n",
      " \"libraries like OpenAI or Hugging Face Transformers. I'm thinking that this \"\n",
      " 'framework would be used to create models that can generate text, maybe '\n",
      " 'answer questions based on those texts, or analyze conversations.\\n'\n",
      " '\\n'\n",
      " \"I'm a bit fuzzy on how exactly these components work together. For instance, \"\n",
      " 'when you load a model into LangChain, does it automatically handle '\n",
      " 'preprocessing the text? Like tokenizing words and handling embeddings? And '\n",
      " 'then, once trained, how do you use that model to generate responses? I '\n",
      " \"suppose there's a specific API or method for that.\\n\"\n",
      " '\\n'\n",
      " 'I wonder if LangChain is more feature-rich than other libraries. For '\n",
      " 'example, maybe it includes methods for data augmentation, which would help '\n",
      " 'in improving the diversity of generated texts by altering inputs in '\n",
      " 'meaningful ways. Or perhaps it has better handling of different language '\n",
      " 'models available on platforms like Hugging Face Spaces.\\n'\n",
      " '\\n'\n",
      " \"Another thing I'm thinking about is how easy it is to deploy a model using \"\n",
      " 'LangChain. Does it provide tools for logging and monitoring? Like setting up '\n",
      " 'metrics or visualizations to track model performance? That would be really '\n",
      " 'useful for deploying in production environments where real-time data can '\n",
      " \"affect the model's output.\\n\"\n",
      " '\\n'\n",
      " 'I also recall that other approaches like using models directly with '\n",
      " 'libraries might involve more manual steps, such as preprocessing text '\n",
      " 'outside of LangChain. So why go through all that when LangChain is supposed '\n",
      " 'to handle most of the complexities? Is it because it abstracts away a lot of '\n",
      " 'manual tasks, making the code cleaner and easier to maintain?\\n'\n",
      " '\\n'\n",
      " \"I'm curious about how far the current framework extends. If it's updated \"\n",
      " 'often with new models from platforms like OpenAI or Hugging Face, does that '\n",
      " 'mean LangChain is getting better each year? Or are there specific versions '\n",
      " 'that cater to particular use cases?\\n'\n",
      " '\\n'\n",
      " \"Moreover, I'm thinking about practical applications. For instance, a project \"\n",
      " 'where an AI needs to answer questions based on the text generated by another '\n",
      " 'model. How would LangChain facilitate that interaction? Would it handle '\n",
      " 'conversation starters or prompts for the next question automatically, or '\n",
      " 'does one need to manually script that part?\\n'\n",
      " '\\n'\n",
      " \"It's also interesting to see how LangChain integrates with other libraries \"\n",
      " \"and frameworks. For example, if I'm using PyTorch alongside LangChain, can I \"\n",
      " \"leverage PyTorch's functionalities while working with models from LangChain? \"\n",
      " 'Or do they have separate modules for different purposes?\\n'\n",
      " '\\n'\n",
      " 'I suppose the key takeaway is that LangChain simplifies building language '\n",
      " 'model applications by providing a comprehensive set of tools and components. '\n",
      " 'It abstracts away many of the tedious tasks, allowing developers to focus on '\n",
      " 'the high-level logic of their applications. This could lead to more '\n",
      " 'efficient, scalable, and maintainable systems compared to implementing '\n",
      " 'everything from scratch.\\n'\n",
      " '\\n'\n",
      " \"However, I'm also thinking about potential limitations. For example, if \"\n",
      " \"someone has a very specific need that isn't covered by LangChain's built-in \"\n",
      " 'features, they might have to implement custom models or preprocess data '\n",
      " 'manually. That could introduce complexity and time into the development '\n",
      " \"process, which isn't always ideal for quick projects or rapid prototyping.\\n\"\n",
      " '\\n'\n",
      " 'Overall, I think I get the gist of LangChain as an API that simplifies '\n",
      " 'building with language models by providing a structured approach with tools '\n",
      " 'for model setup, data handling, interaction, and deployment. It might be a '\n",
      " 'good starting point for someone looking to build applications around '\n",
      " 'language models without getting bogged down in manual tasks and '\n",
      " 'dependencies.\\n'\n",
      " '</think>\\n'\n",
      " '\\n'\n",
      " 'LangChain is a Python framework designed to simplify the development of '\n",
      " 'applications involving language models. It provides a structured approach '\n",
      " 'with key components:\\n'\n",
      " '\\n'\n",
      " '1. **Base Model Class**: Manages different language model architectures, '\n",
      " 'including models from OpenAI and Hugging Face Transformers.\\n'\n",
      " '\\n'\n",
      " '2. **Data Loading Utilities**: Offers tools for preprocessing text, such as '\n",
      " 'tokenization and embedding generation, preparing data for model training.\\n'\n",
      " '\\n'\n",
      " '3. **Model Interfaces**: Facilitates interaction with models through APIs or '\n",
      " 'custom frameworks, enabling tasks like generating responses and analyzing '\n",
      " 'conversations.\\n'\n",
      " '\\n'\n",
      " '4. **Main Module**: Contains utility functions, metrics, and logging methods '\n",
      " 'to manage model performance and deployment.\\n'\n",
      " '\\n'\n",
      " 'LangChain is useful for applications requiring text generation, question '\n",
      " 'answering, and conversational analysis. It abstracts away many manual tasks, '\n",
      " 'making it easier to develop efficient systems. However, potential '\n",
      " 'limitations include the need for custom models or data handling if specific '\n",
      " \"needs aren't covered by its built-in features. Overall, LangChain offers a \"\n",
      " 'clean, feature-rich solution that simplifies development with a focus on '\n",
      " 'high-level logic in applications.')\n"
     ]
    }
   ],
   "source": [
    "question=\"What is LangChain?\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "print(type(response))\n",
    "pprint(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7a11e5",
   "metadata": {},
   "source": [
    "## qwen 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38c060e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db677a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파이썬 (Python)은 하나의 프로그래밍 언어입니다. 이는 1980년대에 인디아의 컴퓨터 소프트웨어 전문 기업인 오페라에 의해 창발된 것입니다.\n",
      "\n",
      "파이썬은 다음과 같은 특징을 가지고 있습니다:\n",
      "\n",
      "1. 간결한 문법: 파이썬의 가장 큰 장점 중 하나는 그 가독성이 뛰어난다는 것입니다. 간단한 문장으로도 많은 작업을 수행할 수 있어, 개발자들의 편의성을 크게 높입니다.\n",
      "\n",
      "2. 넓은 범위: 다양한 분야에서 활용 가능합니다. 웹 프로그래밍, 데이터 분석, 앱 개발, 게임 제작 등에 사용되는 기술이 있습니다.\n",
      "\n",
      "3. 무료로 사용할 수 있음: 이 언어는 오퍼라가 공식적으로 승인한 소프트웨어의 한 형태로 존재합니다. 이것은 파이썬을 무료로 이용하는 데 장점입니다.\n",
      "\n",
      "4. 대량의 자료 풀: 파이썬은 많은 오픈소스 모듈과 패키지에 접근할 수 있는 데 도움을 주므로, 개발자들은 다양한 기능을 쉽게 구현할 수 있습니다.\n",
      "\n",
      "5. 간단한 프로그래밍 훌륭: 파이썬은 특히 시작하는 신비하게 작은 프로젝트에서 큰 성공을 가져다주는 도구입니다.\n",
      "\n",
      "파이썬은 이제 전 세계적으로 웹 개발, 블록체인 기술 등에 활용되는 중요한 언어 중 하나로 자리매김하고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# Ollama를 사용하여 로컬에서 실행 중인 llama3.2 모델 로드\n",
    "model = ChatOllama(model=\"qwen2.5:1.5b\")\n",
    "\n",
    "# 더 정확한 응답을 위한 개선된 프롬프트\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that provides accurate and detailed answers.\"),\n",
    "    (\"human\", \"Q: {question}\\nA:\")\n",
    "])\n",
    "\n",
    "# 최신 LangChain 방식: RunnableSequence 활용\n",
    "chain = prompt_template | model\n",
    "\n",
    "# 실행 예시\n",
    "question = \"파이썬은 무엇인가요?\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-farQSE-J-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
